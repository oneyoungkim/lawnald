"""
ë³€í˜¸ì‚¬ ì´ë©”ì¼ ìˆ˜ì§‘ í¬ë¡¤ëŸ¬ v2
- ëŒ€í•œë³€í˜¸ì‚¬í˜‘íšŒ íšŒì› ê²€ìƒ‰
- ë„¤ì´ë²„ ë¸”ë¡œê·¸: mainFrame ë‚´ë¶€ ë³¸ë¬¸ + ë²•ë¥  í‚¤ì›Œë“œ ì „ëµ
- ìœ íŠœë¸Œ: Description/ê³ ì • ëŒ“ê¸€ + êµ¬ë…ì ìˆ˜ ê¸°ë°˜ ìš°ì„ ìˆœìœ„
- í‚¤ì›Œë“œë³„ ìë™ íƒœê¹… (#ì „ì„¸ì‚¬ê¸°, #ì´í˜¼ ë“±)
- Anti-blocking: time.sleep, User-Agent ë¡œí…Œì´ì…˜, ì¬ì‹œë„ ë¡œì§
"""
import json
import os
import re
import time
import random
import hashlib
from datetime import datetime, date
from typing import List, Dict, Optional

import requests
from bs4 import BeautifulSoup

# â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DB_PATH = os.path.join(os.path.dirname(__file__), "lawyer_contacts_db.json")

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0",
]

EMAIL_REGEX = re.compile(r"[a-zA-Z0-9._%+\-]+@[a-zA-Z0-9.\-]+\.[a-zA-Z]{2,}")

# â”€â”€ ë²•ë¥  ë¶„ì•¼ë³„ í‚¤ì›Œë“œ ë§µ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LEGAL_KEYWORDS = {
    "ì´í˜¼": ["ì´í˜¼ ë³€í˜¸ì‚¬", "ì´í˜¼ì†Œì†¡ ë³€í˜¸ì‚¬", "ì–‘ìœ¡ê¶Œ ë³€í˜¸ì‚¬", "ì¬ì‚°ë¶„í•  ë³€í˜¸ì‚¬", "í˜‘ì˜ì´í˜¼ ë³€í˜¸ì‚¬", "ìœ„ìë£Œ ë³€í˜¸ì‚¬"],
    "ì „ì„¸ì‚¬ê¸°": ["ì „ì„¸ì‚¬ê¸° ë³€í˜¸ì‚¬", "ì „ì„¸ë³´ì¦ê¸ˆ ë³€í˜¸ì‚¬", "ì„ëŒ€ì°¨ ë³€í˜¸ì‚¬", "ì „ì„¸ í”¼í•´ ë³€í˜¸ì‚¬", "ë³´ì¦ê¸ˆ ë°˜í™˜ ë³€í˜¸ì‚¬"],
    "í˜•ì‚¬": ["í˜•ì‚¬ ë³€í˜¸ì‚¬", "ì„±ë²”ì£„ ë³€í˜¸ì‚¬", "ìŒì£¼ìš´ì „ ë³€í˜¸ì‚¬", "í­í–‰ ë³€í˜¸ì‚¬", "ì‚¬ê¸°ì£„ ë³€í˜¸ì‚¬", "ë§ˆì•½ ë³€í˜¸ì‚¬"],
    "ë¶€ë™ì‚°": ["ë¶€ë™ì‚° ë³€í˜¸ì‚¬", "ëª…ë„ì†Œì†¡ ë³€í˜¸ì‚¬", "ë¶€ë™ì‚° ë¶„ìŸ ë³€í˜¸ì‚¬", "ê±´ì¶• ë³€í˜¸ì‚¬", "ì¬ê°œë°œ ë³€í˜¸ì‚¬"],
    "ìƒì†": ["ìƒì† ë³€í˜¸ì‚¬", "ìœ ì‚° ë¶„ìŸ ë³€í˜¸ì‚¬", "ìƒì†ì„¸ ë³€í˜¸ì‚¬", "ìœ ì–¸ì¥ ë³€í˜¸ì‚¬", "ìƒì†í¬ê¸° ë³€í˜¸ì‚¬"],
    "ë…¸ë™": ["ë…¸ë™ ë³€í˜¸ì‚¬", "ë¶€ë‹¹í•´ê³  ë³€í˜¸ì‚¬", "ì„ê¸ˆì²´ë¶ˆ ë³€í˜¸ì‚¬", "ê·¼ë¡œê³„ì•½ ë³€í˜¸ì‚¬", "ì‚°ì¬ ë³€í˜¸ì‚¬"],
    "êµí†µì‚¬ê³ ": ["êµí†µì‚¬ê³  ë³€í˜¸ì‚¬", "êµí†µì‚¬ê³  í•©ì˜ ë³€í˜¸ì‚¬", "ìë™ì°¨ ì‚¬ê³  ë³€í˜¸ì‚¬", "ëº‘ì†Œë‹ˆ ë³€í˜¸ì‚¬"],
    "ì˜ë£Œ": ["ì˜ë£Œì‚¬ê³  ë³€í˜¸ì‚¬", "ì˜ë£Œê³¼ì‹¤ ë³€í˜¸ì‚¬", "ì˜ë£Œë¶„ìŸ ë³€í˜¸ì‚¬", "ì˜ë£Œì†Œì†¡ ë³€í˜¸ì‚¬"],
    "ë¯¼ì‚¬": ["ë¯¼ì‚¬ì†Œì†¡ ë³€í˜¸ì‚¬", "ì†í•´ë°°ìƒ ë³€í˜¸ì‚¬", "ì±„ê¶Œì¶”ì‹¬ ë³€í˜¸ì‚¬", "ë¯¼ì‚¬ ë¶„ìŸ ë³€í˜¸ì‚¬"],
    "ê¸°ì—…": ["ê¸°ì—… ë²•ë¬´ ë³€í˜¸ì‚¬", "ë²•ì¸ ë³€í˜¸ì‚¬", "ê¸°ì—… ì†Œì†¡ ë³€í˜¸ì‚¬", "ê³„ì•½ì„œ ê²€í†  ë³€í˜¸ì‚¬", "ìŠ¤íƒ€íŠ¸ì—… ë³€í˜¸ì‚¬"],
}

# ì´ë©”ì¼ ì œì™¸ ë„ë©”ì¸ (gmail.comì€ ì œì™¸í•˜ì§€ ì•ŠìŒ â€” ë§ì€ ë³€í˜¸ì‚¬ê°€ Gmail ì‚¬ìš©)
EXCLUDED_EMAIL_DOMAINS = [
    "noreply", "example.com", "navercorp", "naver.com",
    "google.com", "youtube.com", "daum.net",
    "hanmail.net", "kakao.com", "test.com"
]

# ì „í™”ë²ˆí˜¸ ì¶”ì¶œ ì •ê·œì‹
PHONE_REGEX = re.compile(r"(0\d{1,2}[-.\s]?\d{3,4}[-.\s]?\d{4})")


# â”€â”€ ìœ í‹¸ë¦¬í‹° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _random_sleep(min_sec: float = 3.0, max_sec: float = 7.0):
    """ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ëœë¤ ë”œë ˆì´"""
    delay = random.uniform(min_sec, max_sec)
    time.sleep(delay)
    return delay


def _get_headers() -> dict:
    """ëœë¤ User-Agent í—¤ë” ë°˜í™˜"""
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
        "Accept-Encoding": "gzip, deflate",
        "Connection": "keep-alive",
    }


def _safe_request(url: str, method: str = "GET", max_retries: int = 3, **kwargs) -> Optional[requests.Response]:
    """ë°±ì˜¤í”„ ì¬ì‹œë„ê°€ í¬í•¨ëœ ì•ˆì „í•œ HTTP ìš”ì²­"""
    for attempt in range(max_retries):
        try:
            headers = _get_headers()
            if "headers" in kwargs:
                headers.update(kwargs.pop("headers"))
            resp = requests.request(method, url, headers=headers, timeout=15, **kwargs)
            resp.raise_for_status()
            return resp
        except requests.exceptions.RequestException as e:
            wait = (attempt + 1) * 5 + random.uniform(1, 3)
            print(f"  [ì¬ì‹œë„ {attempt+1}/{max_retries}] {e} â†’ {wait:.1f}ì´ˆ ëŒ€ê¸°")
            time.sleep(wait)
    return None


def _generate_id(name: str, email: str) -> str:
    raw = f"{name}_{email}".lower()
    return hashlib.md5(raw.encode()).hexdigest()[:12]


def _is_valid_lawyer_email(email: str) -> bool:
    """ë³€í˜¸ì‚¬ ì—…ë¬´ìš©ìœ¼ë¡œ ë³´ì´ëŠ” ì´ë©”ì¼ì¸ì§€ íŒë‹¨"""
    lower = email.lower()
    for excluded in EXCLUDED_EMAIL_DOMAINS:
        if excluded in lower:
            return False
    # ë„ˆë¬´ ì§§ì€ ì´ë©”ì¼ ì œì™¸ (spam ë°©ì§€)
    if len(lower) < 5:
        return False
    return True


def _ensure_lawyer_keyword(keyword: str) -> str:
    """í‚¤ì›Œë“œì— 'ë³€í˜¸ì‚¬'ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ìë™ ì¶”ê°€"""
    if "ë³€í˜¸ì‚¬" not in keyword and "ë²•ë¬´" not in keyword and "ë¡œíŒ" not in keyword:
        return f"{keyword} ë³€í˜¸ì‚¬"
    return keyword


def _format_subscribers(count: int) -> str:
    """êµ¬ë…ì ìˆ˜ í¬ë§·íŒ…"""
    if count >= 10000:
        return f"{count // 10000}ë§Œ"
    elif count >= 1000:
        return f"{count // 1000}ì²œ"
    return str(count)


# â”€â”€ DB ê´€ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_contacts() -> List[Dict]:
    if os.path.exists(DB_PATH):
        try:
            with open(DB_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            return []
    return []


def save_contacts(contacts: List[Dict]):
    with open(DB_PATH, "w", encoding="utf-8") as f:
        json.dump(contacts, f, ensure_ascii=False, indent=2)


def add_contacts(new_contacts: List[Dict]) -> dict:
    """ì¤‘ë³µ ì œê±° í›„ ì—°ë½ì²˜ ì¶”ê°€, í†µê³„ ë°˜í™˜"""
    existing = load_contacts()
    existing_emails = {c["email"].lower() for c in existing if c.get("email")}

    added = 0
    skipped = 0
    for contact in new_contacts:
        email = contact.get("email", "").lower().strip()
        if not email or email in existing_emails:
            skipped += 1
            continue
        contact["id"] = _generate_id(contact.get("name", ""), email)
        contact["collected_at"] = datetime.now().isoformat()
        existing.append(contact)
        existing_emails.add(email)
        added += 1

    save_contacts(existing)
    return {"added": added, "skipped": skipped, "total": len(existing)}


def get_today_count() -> int:
    """ì˜¤ëŠ˜ ìˆ˜ì§‘ëœ ì—°ë½ì²˜ ìˆ˜"""
    contacts = load_contacts()
    today_str = date.today().isoformat()
    return sum(1 for c in contacts if c.get("collected_at", "").startswith(today_str))


# â”€â”€ í¬ë¡¤ëŸ¬: ëŒ€í•œë³€í˜¸ì‚¬í˜‘íšŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class KoreanBarCrawler:
    """ëŒ€í•œë³€í˜¸ì‚¬í˜‘íšŒ(koreanbar.or.kr) ë³€í˜¸ì‚¬ ê²€ìƒ‰ í¬ë¡¤ëŸ¬"""
    BASE_URL = "https://www.koreanbar.or.kr"
    SEARCH_URL = f"{BASE_URL}/pages/search/EmpSchPage.aspx"

    def crawl(self, max_pages: int = 5, keyword: str = "", tags: Optional[List[str]] = None) -> List[Dict]:
        results = []
        print(f"\n{'='*50}")
        print(f"[ëŒ€í•œë³€í˜‘ í¬ë¡¤ëŸ¬] ì‹œì‘ (max_pages={max_pages}, keyword='{keyword}')")
        print(f"{'='*50}")

        for page in range(1, max_pages + 1):
            print(f"\n  ğŸ“„ í˜ì´ì§€ {page}/{max_pages} ìˆ˜ì§‘ ì¤‘...")
            delay = _random_sleep(3.0, 7.0)
            print(f"  â³ {delay:.1f}ì´ˆ ë”œë ˆì´ ì ìš©")

            try:
                params = {"page": page}
                if keyword:
                    params["searchWord"] = keyword

                resp = _safe_request(self.SEARCH_URL, params=params)
                if not resp:
                    print(f"  âŒ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨, ê±´ë„ˆëœ€")
                    continue

                soup = BeautifulSoup(resp.text, "html.parser")
                rows = soup.select("table.list_table tbody tr")
                if not rows:
                    rows = soup.select(".search_result .item")
                if not rows:
                    rows = soup.select("table tr")

                page_count = 0
                for row in rows:
                    contact = self._parse_row(row)
                    if contact and contact.get("email"):
                        contact["source"] = "ëŒ€í•œë³€í˜¸ì‚¬í˜‘íšŒ"
                        contact["tags"] = tags or []
                        results.append(contact)
                        page_count += 1

                print(f"  âœ… í˜ì´ì§€ {page}: {page_count}ê±´ ìˆ˜ì§‘")
                if page_count == 0 and page > 1:
                    break

            except Exception as e:
                print(f"  âŒ í˜ì´ì§€ {page} ì˜¤ë¥˜: {e}")
                continue

        print(f"\n[ëŒ€í•œë³€í˜‘ í¬ë¡¤ëŸ¬] ì™„ë£Œ: ì´ {len(results)}ê±´ ìˆ˜ì§‘")
        return results

    def _parse_row(self, row) -> Optional[Dict]:
        try:
            cells = row.find_all("td")
            if len(cells) < 2:
                return None

            text = row.get_text(" ", strip=True)
            emails = EMAIL_REGEX.findall(text)
            if not emails:
                mailto = row.find("a", href=re.compile(r"^mailto:"))
                if mailto:
                    email_match = EMAIL_REGEX.search(mailto["href"])
                    if email_match:
                        emails = [email_match.group()]
            if not emails:
                return None

            name_el = row.find("strong") or row.find("a") or cells[0]
            name = name_el.get_text(strip=True) if name_el else ""
            firm_el = row.find(class_=re.compile(r"firm|office|belong")) or (cells[1] if len(cells) > 1 else None)
            firm = firm_el.get_text(strip=True) if firm_el else ""

            return {"name": name, "firm": firm, "email": emails[0]}
        except Exception:
            return None


# â”€â”€ í¬ë¡¤ëŸ¬: ë„¤ì´ë²„ ë¸”ë¡œê·¸ (mainFrame ì „ëµ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class NaverBlogCrawler:
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v2
    - ë²•ë¥  ë¶„ì•¼ë³„ í‚¤ì›Œë“œë¡œ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰
    - ê° ë¸”ë¡œê·¸ì˜ mainFrame(PostView) ë‚´ë¶€ ë³¸ë¬¸ ì§ì ‘ ì ‘ê·¼
    - í•˜ë‹¨ ì—°ë½ì²˜ ì„¹ì…˜ì—ì„œ @ í¬í•¨ ì´ë©”ì¼ ì£¼ì†Œ ìˆ˜ì§‘
    - ê²€ìƒ‰ í‚¤ì›Œë“œ ê¸°ë°˜ ìë™ íƒœê¹…
    """
    SEARCH_URL = "https://search.naver.com/search.naver"

    def crawl(self, keywords: Optional[List[str]] = None,
              max_results_per_keyword: int = 10,
              legal_categories: Optional[List[str]] = None) -> List[Dict]:
        results = []

        # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ or ì§ì ‘ í‚¤ì›Œë“œ ì‚¬ìš©
        if legal_categories:
            keyword_map = {}
            for cat in legal_categories:
                if cat in LEGAL_KEYWORDS:
                    for kw in LEGAL_KEYWORDS[cat]:
                        keyword_map[kw] = cat
            search_items = list(keyword_map.items())
        elif keywords:
            # ì‚¬ìš©ì í‚¤ì›Œë“œì— 'ë³€í˜¸ì‚¬' ìë™ ì¶”ê°€
            search_items = [(_ensure_lawyer_keyword(kw), kw) for kw in keywords]
        else:
            # ê¸°ë³¸: ëª¨ë“  ë²•ë¥  ì¹´í…Œê³ ë¦¬
            keyword_map = {}
            for cat, kws in LEGAL_KEYWORDS.items():
                for kw in kws[:2]:  # ì¹´í…Œê³ ë¦¬ë‹¹ 2ê°œ í‚¤ì›Œë“œ
                    keyword_map[kw] = cat
            search_items = list(keyword_map.items())

        print(f"\n{'='*50}")
        print(f"[ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v2] ì‹œì‘ ({len(search_items)}ê°œ í‚¤ì›Œë“œ)")
        print(f"{'='*50}")

        for idx, (keyword, tag) in enumerate(search_items, 1):
            print(f"\n  ğŸ” [{idx}/{len(search_items)}] í‚¤ì›Œë“œ: '{keyword}' â†’ íƒœê·¸: #{tag}")

            delay = _random_sleep(4.0, 8.0)
            print(f"  â³ {delay:.1f}ì´ˆ ë”œë ˆì´ ì ìš©")

            try:
                # ë„¤ì´ë²„ ë¸”ë¡œê·¸ íƒ­ ê²€ìƒ‰
                params = {
                    "where": "blog",
                    "query": keyword,
                    "sm": "tab_opt",
                    "nso": "",
                }
                resp = _safe_request(self.SEARCH_URL, params=params)
                if not resp:
                    print(f"  âŒ ê²€ìƒ‰ ì‹¤íŒ¨, ê±´ë„ˆëœ€")
                    continue

                soup = BeautifulSoup(resp.text, "html.parser")

                # ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ë§í¬ ì¶”ì¶œ (ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„)
                blog_links = []
                # ì…€ë ‰í„° 1: ì¼ë°˜ì ì¸ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼
                for a_tag in soup.select("a.api_txt_lines.total_tit"):
                    href = a_tag.get("href", "")
                    if href and "blog.naver.com" in href:
                        blog_links.append(href)

                # ì…€ë ‰í„° 2: ëŒ€ì²´ êµ¬ì¡°
                if not blog_links:
                    for a_tag in soup.select(".total_wrap a[href*='blog.naver.com']"):
                        href = a_tag.get("href", "")
                        if href:
                            blog_links.append(href)

                # ì…€ë ‰í„° 3: ë” ë„“ì€ ë²”ìœ„ - ëª¨ë“  ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë§í¬
                if not blog_links:
                    for a_tag in soup.find_all("a", href=True):
                        href = a_tag["href"]
                        if "blog.naver.com" in href and href not in blog_links:
                            blog_links.append(href)

                # ì¤‘ë³µ ì œê±° ë° ì œí•œ
                blog_links = list(dict.fromkeys(blog_links))[:max_results_per_keyword]
                print(f"  ğŸ“‹ ë¸”ë¡œê·¸ ë§í¬ {len(blog_links)}ê°œ ë°œê²¬")

                for link in blog_links:
                    delay = _random_sleep(3.0, 6.0)
                    contact = self._extract_from_blog_mainframe(link)
                    if contact and contact.get("email"):
                        contact["source"] = "ë„¤ì´ë²„ ë¸”ë¡œê·¸"
                        contact["source_url"] = link
                        contact["tags"] = [tag]
                        contact["search_keyword"] = keyword
                        results.append(contact)
                        print(f"    âœ… ìˆ˜ì§‘: {contact['name']} ({contact['email']}) #{tag}")
                    else:
                        print(f"    â¬œ ì´ë©”ì¼ ë¯¸ë°œê²¬: {link[:50]}...")

            except Exception as e:
                print(f"  âŒ í‚¤ì›Œë“œ '{keyword}' ì˜¤ë¥˜: {e}")
                continue

        print(f"\n[ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ v2] ì™„ë£Œ: ì´ {len(results)}ê±´ ìˆ˜ì§‘")
        return results

    def _extract_from_blog_mainframe(self, url: str) -> Optional[Dict]:
        """
        ë„¤ì´ë²„ ë¸”ë¡œê·¸ mainFrame(PostView) ë‚´ë¶€ ë³¸ë¬¸ì„ ì§ì ‘ ì ‘ê·¼í•˜ì—¬ ì´ë©”ì¼ ì¶”ì¶œ.
        ë„¤ì´ë²„ ë¸”ë¡œê·¸ëŠ” iframe êµ¬ì¡° â†’ PostView.naver URLë¡œ ì§ì ‘ ì ‘ê·¼.
        """
        try:
            # URLì—ì„œ blogIdì™€ logNo ì¶”ì¶œ
            blog_id = None
            log_no = None

            # https://blog.naver.com/blogId/logNo íŒ¨í„´
            match = re.search(r"blog\.naver\.com/([^/?]+)/(\d+)", url)
            if match:
                blog_id = match.group(1)
                log_no = match.group(2)

            if not blog_id or not log_no:
                # ëª¨ë°”ì¼ URL ì‹œë„
                match = re.search(r"m\.blog\.naver\.com/([^/?]+)/(\d+)", url)
                if match:
                    blog_id = match.group(1)
                    log_no = match.group(2)

            if blog_id and log_no:
                # mainFrame(PostView) ì§ì ‘ ì ‘ê·¼
                post_url = f"https://blog.naver.com/PostView.naver?blogId={blog_id}&logNo={log_no}"
                resp = _safe_request(post_url)
            else:
                # fallback: ëª¨ë°”ì¼ ë²„ì „
                mobile_url = url.replace("blog.naver.com", "m.blog.naver.com")
                resp = _safe_request(mobile_url)

            if not resp:
                return None

            soup = BeautifulSoup(resp.text, "html.parser")

            # mainFrame ë³¸ë¬¸ ì˜ì—­ ì„ íƒ (ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„)
            content_area = (
                soup.select_one(".se-main-container") or      # SmartEditor 3
                soup.select_one("#postViewArea") or            # PostView
                soup.select_one(".post-view") or               # ëª¨ë°”ì¼
                soup.select_one("#post-view") or
                soup
            )

            text = content_area.get_text(" ", strip=True) if content_area else ""

            # ì „ì²´ í˜ì´ì§€ í…ìŠ¤íŠ¸ë„ ë³´ì¡° ì‚¬ìš©
            full_text = soup.get_text(" ", strip=True)

            # ì´ë©”ì¼ ì¶”ì¶œ (ë³¸ë¬¸ + ì „ì²´ì—ì„œ)
            emails_body = EMAIL_REGEX.findall(text)
            emails_full = EMAIL_REGEX.findall(full_text)
            all_emails = list(dict.fromkeys(emails_body + emails_full))  # ìˆœì„œ ìœ ì§€ ì¤‘ë³µ ì œê±°

            # ìœ íš¨í•œ ë³€í˜¸ì‚¬ ì´ë©”ì¼ë§Œ í•„í„°
            valid_emails = [e for e in all_emails if _is_valid_lawyer_email(e)]

            if not valid_emails:
                return None

            # ë³€í˜¸ì‚¬ ì´ë¦„ ì¶”ì¶œ
            name = ""
            name_match = re.search(r"([ê°€-í£]{2,4})\s*ë³€í˜¸ì‚¬", text) or re.search(r"([ê°€-í£]{2,4})\s*ë³€í˜¸ì‚¬", full_text)
            if name_match:
                name = name_match.group(1)

            # ë²•ë¬´ë²•ì¸/ë²•ë¥ ì‚¬ë¬´ì†Œ ì¶”ì¶œ
            firm = ""
            firm_match = re.search(r"(ë²•ë¬´ë²•ì¸|ë²•ë¥ ì‚¬ë¬´ì†Œ|ë¡œíŒ)\s*[ê°€-í£\w]{1,10}", text) or \
                         re.search(r"(ë²•ë¬´ë²•ì¸|ë²•ë¥ ì‚¬ë¬´ì†Œ|ë¡œíŒ)\s*[ê°€-í£\w]{1,10}", full_text)
            if firm_match:
                firm = firm_match.group(0)

            # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ (ë³´ì¡° ì—°ë½ì²˜)
            phone = ""
            phone_match = PHONE_REGEX.search(text) or PHONE_REGEX.search(full_text)
            if phone_match:
                phone = phone_match.group(1)

            # ë¸”ë¡œê·¸ ì œëª©ì—ì„œ ë³´ì™„
            title_el = soup.find("title")
            if title_el and not name:
                title_text = title_el.get_text(strip=True)
                title_name = re.search(r"([ê°€-í£]{2,4})\s*ë³€í˜¸ì‚¬", title_text)
                if title_name:
                    name = title_name.group(1)

            result = {
                "name": name or "ë¯¸í™•ì¸",
                "firm": firm,
                "email": valid_emails[0],
            }
            if phone:
                result["phone"] = phone
            return result

        except Exception:
            return None


# â”€â”€ í¬ë¡¤ëŸ¬: ìœ íŠœë¸Œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class YouTubeCrawler:
    """
    ìœ íŠœë¸Œ ë²•ë¥  ì±„ë„ í¬ë¡¤ëŸ¬
    - ë²•ë¥  í‚¤ì›Œë“œë¡œ ì˜ìƒ ê²€ìƒ‰
    - ì˜ìƒ Description(ì„¤ëª…ë€) + ê³ ì • ëŒ“ê¸€ì—ì„œ ì´ë©”ì¼ ì¶”ì¶œ
    - ì±„ë„ êµ¬ë…ì ìˆ˜ ìˆ˜ì§‘ â†’ ì˜í–¥ë ¥ ê¸°ë°˜ ìš°ì„ ìˆœìœ„
    - í‚¤ì›Œë“œë³„ ìë™ íƒœê¹…
    """
    SEARCH_URL = "https://www.youtube.com/results"

    def crawl(self, keywords: Optional[List[str]] = None,
              max_results_per_keyword: int = 10,
              legal_categories: Optional[List[str]] = None) -> List[Dict]:
        results = []

        # í‚¤ì›Œë“œ ì¤€ë¹„
        if legal_categories:
            keyword_map = {}
            for cat in legal_categories:
                if cat in LEGAL_KEYWORDS:
                    for kw in LEGAL_KEYWORDS[cat]:
                        keyword_map[kw] = cat
            search_items = list(keyword_map.items())
        elif keywords:
            # ì‚¬ìš©ì í‚¤ì›Œë“œì— 'ë³€í˜¸ì‚¬' ìë™ ì¶”ê°€
            search_items = [(_ensure_lawyer_keyword(kw), kw) for kw in keywords]
        else:
            keyword_map = {}
            for cat, kws in LEGAL_KEYWORDS.items():
                keyword_map[kws[0]] = cat  # ì¹´í…Œê³ ë¦¬ë‹¹ 1ê°œ
            search_items = list(keyword_map.items())

        print(f"\n{'='*50}")
        print(f"[ìœ íŠœë¸Œ í¬ë¡¤ëŸ¬] ì‹œì‘ ({len(search_items)}ê°œ í‚¤ì›Œë“œ)")
        print(f"{'='*50}")

        seen_channels = set()

        for idx, (keyword, tag) in enumerate(search_items, 1):
            print(f"\n  ğŸ¬ [{idx}/{len(search_items)}] í‚¤ì›Œë“œ: '{keyword}' â†’ íƒœê·¸: #{tag}")

            delay = _random_sleep(4.0, 8.0)
            print(f"  â³ {delay:.1f}ì´ˆ ë”œë ˆì´ ì ìš©")

            try:
                # ìœ íŠœë¸Œ ê²€ìƒ‰
                params = {"search_query": keyword}
                resp = _safe_request(self.SEARCH_URL, params=params)
                if not resp:
                    print(f"  âŒ ê²€ìƒ‰ ì‹¤íŒ¨, ê±´ë„ˆëœ€")
                    continue

                # ìœ íŠœë¸Œ ê²€ìƒ‰ê²°ê³¼ì—ì„œ ì˜ìƒ ID ì¶”ì¶œ (HTMLì—ì„œ íŒŒì‹±)
                video_ids = re.findall(r'"videoId":"([a-zA-Z0-9_-]{11})"', resp.text)
                video_ids = list(dict.fromkeys(video_ids))[:max_results_per_keyword]
                print(f"  ğŸ“‹ ì˜ìƒ {len(video_ids)}ê°œ ë°œê²¬")

                for vid in video_ids:
                    delay = _random_sleep(3.0, 6.0)
                    contact = self._extract_from_video(vid, tag)
                    if contact and contact.get("email"):
                        # ì±„ë„ ì¤‘ë³µ ë°©ì§€
                        channel_key = contact.get("youtube_channel", "")
                        if channel_key and channel_key in seen_channels:
                            print(f"    â¬œ ì´ë¯¸ ìˆ˜ì§‘ëœ ì±„ë„: {channel_key}")
                            continue
                        if channel_key:
                            seen_channels.add(channel_key)

                        contact["source"] = "ìœ íŠœë¸Œ"
                        contact["source_url"] = f"https://youtube.com/watch?v={vid}"
                        # ê¸°ì¡´ íƒœê·¸ì— ì¶”ê°€
                        existing_tags = contact.get("tags", [])
                        if tag not in existing_tags:
                            existing_tags.append(tag)
                        contact["tags"] = existing_tags
                        contact["search_keyword"] = keyword
                        results.append(contact)

                        subs_str = _format_subscribers(contact.get("subscribers", 0))
                        print(f"    âœ… ìˆ˜ì§‘: {contact['name']} ({contact['email']}) êµ¬ë…ì:{subs_str} #{tag}")
                    else:
                        print(f"    â¬œ ì´ë©”ì¼ ë¯¸ë°œê²¬: /watch?v={vid}")

            except Exception as e:
                print(f"  âŒ í‚¤ì›Œë“œ '{keyword}' ì˜¤ë¥˜: {e}")
                continue

        # êµ¬ë…ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        results.sort(key=lambda x: x.get("subscribers", 0), reverse=True)
        print(f"\n[ìœ íŠœë¸Œ í¬ë¡¤ëŸ¬] ì™„ë£Œ: ì´ {len(results)}ê±´ ìˆ˜ì§‘ (êµ¬ë…ì ìˆ˜ ê¸°ì¤€ ì •ë ¬)")
        return results

    def _extract_from_video(self, video_id: str, tag: str = "") -> Optional[Dict]:
        """ì˜ìƒì˜ Description + ê³ ì • ëŒ“ê¸€ì—ì„œ ë³€í˜¸ì‚¬ ì´ë©”ì¼ ì¶”ì¶œ"""
        try:
            # ì˜ìƒ í˜ì´ì§€ ì ‘ê·¼
            url = f"https://www.youtube.com/watch?v={video_id}"
            resp = _safe_request(url)
            if not resp:
                return None

            html = resp.text

            # â”€â”€ Description ì¶”ì¶œ (JSON-LD / ytInitialData) â”€â”€
            description = ""
            # ytInitialDataì—ì„œ description ì¶”ì¶œ
            desc_match = re.search(r'"shortDescription":"(.*?)"', html)
            if desc_match:
                description = desc_match.group(1).replace("\\n", "\n").replace("\\t", " ")

            # â”€â”€ ì±„ë„ ì •ë³´ ì¶”ì¶œ â”€â”€
            channel_name = ""
            channel_match = re.search(r'"ownerChannelName":"(.*?)"', html)
            if channel_match:
                channel_name = channel_match.group(1)

            # â”€â”€ êµ¬ë…ì ìˆ˜ ì¶”ì¶œ â”€â”€
            subscribers = 0
            sub_match = re.search(r'"subscriberCountText":\{"simpleText":"êµ¬ë…ì\s*([0-9,.]+)(ë§Œ|ì²œ)?ëª…', html)
            if sub_match:
                num_str = sub_match.group(1).replace(",", "")
                unit = sub_match.group(2) or ""
                try:
                    num = float(num_str)
                    if unit == "ë§Œ":
                        subscribers = int(num * 10000)
                    elif unit == "ì²œ":
                        subscribers = int(num * 1000)
                    else:
                        subscribers = int(num)
                except ValueError:
                    subscribers = 0

            if not sub_match:
                # ëŒ€ì²´ íŒ¨í„´
                sub_match2 = re.search(r'"subscriberCountText":\{"simpleText":"([\d.]+)(ë§Œ|ì²œ)?', html)
                if sub_match2:
                    try:
                        num = float(sub_match2.group(1))
                        unit = sub_match2.group(2) or ""
                        if unit == "ë§Œ":
                            subscribers = int(num * 10000)
                        elif unit == "ì²œ":
                            subscribers = int(num * 1000)
                        else:
                            subscribers = int(num)
                    except ValueError:
                        pass

            # â”€â”€ ê³ ì • ëŒ“ê¸€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„ â”€â”€
            pinned_comment = ""
            pinned_match = re.search(r'"pinnedCommentRenderer".*?"text":\{"runs":\[\{"text":"(.*?)"\}', html)
            if pinned_match:
                pinned_comment = pinned_match.group(1).replace("\\n", "\n")

            # â”€â”€ ì´ë©”ì¼ ì¶”ì¶œ (description + ê³ ì • ëŒ“ê¸€) â”€â”€
            combined_text = f"{description}\n{pinned_comment}"
            all_emails = EMAIL_REGEX.findall(combined_text)
            valid_emails = [e for e in all_emails if _is_valid_lawyer_email(e)]

            if not valid_emails:
                return None

            # ë³€í˜¸ì‚¬ ì´ë¦„ ì¶”ì¶œ
            name = ""
            name_match = re.search(r"([ê°€-í£]{2,4})\s*ë³€í˜¸ì‚¬", combined_text)
            if name_match:
                name = name_match.group(1)
            elif channel_name:
                # ì±„ë„ëª…ì—ì„œ ì¶”ì¶œ ì‹œë„
                ch_name_match = re.search(r"([ê°€-í£]{2,4})\s*ë³€í˜¸ì‚¬", channel_name)
                if ch_name_match:
                    name = ch_name_match.group(1)

            # ë²•ë¬´ë²•ì¸ ì¶”ì¶œ
            firm = ""
            firm_match = re.search(r"(ë²•ë¬´ë²•ì¸|ë²•ë¥ ì‚¬ë¬´ì†Œ|ë¡œíŒ)\s*[ê°€-í£\w]{1,10}", combined_text)
            if firm_match:
                firm = firm_match.group(0)

            return {
                "name": name or channel_name or "ë¯¸í™•ì¸",
                "firm": firm,
                "email": valid_emails[0],
                "youtube_channel": channel_name,
                "subscribers": subscribers,
                "subscribers_display": _format_subscribers(subscribers),
                "tags": [],
            }

        except Exception:
            return None


# â”€â”€ í¬í„¸(ë²•ë¥  í¬íƒˆ) í¬ë¡¤ëŸ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class LegalPortalCrawler:
    """ë²•ë¥  í¬í„¸ ì‚¬ì´íŠ¸ì—ì„œ ê³µê°œëœ ë³€í˜¸ì‚¬ ì´ë©”ì¼ì„ ìˆ˜ì§‘"""
    LAWTALK_URL = "https://www.lawtalk.co.kr"

    def crawl(self, max_pages: int = 3, tags: Optional[List[str]] = None) -> List[Dict]:
        results = []
        print(f"\n{'='*50}")
        print(f"[ë²•ë¥  í¬í„¸ í¬ë¡¤ëŸ¬] ì‹œì‘ (max_pages={max_pages})")
        print(f"{'='*50}")

        for page in range(1, max_pages + 1):
            print(f"\n  ğŸ“„ í˜ì´ì§€ {page}/{max_pages} ìˆ˜ì§‘ ì¤‘...")
            delay = _random_sleep(4.0, 8.0)
            print(f"  â³ {delay:.1f}ì´ˆ ë”œë ˆì´ ì ìš©")

            try:
                url = f"{self.LAWTALK_URL}/lawyers?page={page}"
                resp = _safe_request(url)
                if not resp:
                    print(f"  âŒ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨")
                    continue

                soup = BeautifulSoup(resp.text, "html.parser")
                profile_links = []
                for a_tag in soup.select("a[href*='/lawyers/']"):
                    href = a_tag.get("href", "")
                    if href and "/lawyers/" in href:
                        full_url = self.LAWTALK_URL + href if href.startswith("/") else href
                        if full_url not in profile_links:
                            profile_links.append(full_url)

                profile_links = profile_links[:10]
                print(f"  ğŸ“‹ í”„ë¡œí•„ {len(profile_links)}ê°œ ë°œê²¬")

                for link in profile_links:
                    delay = _random_sleep(3.0, 6.0)
                    contact = self._extract_from_profile(link)
                    if contact and contact.get("email"):
                        contact["source"] = "ë²•ë¥  í¬í„¸"
                        contact["source_url"] = link
                        contact["tags"] = tags or []
                        results.append(contact)
                        print(f"    âœ… ìˆ˜ì§‘: {contact['name']} ({contact['email']})")

            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜: {e}")
                continue

        print(f"\n[ë²•ë¥  í¬í„¸ í¬ë¡¤ëŸ¬] ì™„ë£Œ: ì´ {len(results)}ê±´ ìˆ˜ì§‘")
        return results

    def _extract_from_profile(self, url: str) -> Optional[Dict]:
        try:
            resp = _safe_request(url)
            if not resp:
                return None
            soup = BeautifulSoup(resp.text, "html.parser")
            text = soup.get_text(" ", strip=True)
            emails = EMAIL_REGEX.findall(text)
            filtered = [e for e in emails if _is_valid_lawyer_email(e)]
            if not filtered:
                return None

            name = ""
            name_match = re.search(r"([ê°€-í£]{2,4})\s*ë³€í˜¸ì‚¬", text)
            if name_match:
                name = name_match.group(1)

            firm = ""
            firm_match = re.search(r"(ë²•ë¬´ë²•ì¸|ë²•ë¥ ì‚¬ë¬´ì†Œ)\s*[ê°€-í£\w]+", text)
            if firm_match:
                firm = firm_match.group(0)

            return {"name": name or "ë¯¸í™•ì¸", "firm": firm, "email": filtered[0]}
        except Exception:
            return None


# â”€â”€ ë©”ì¸ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class LawyerCrawler:
    """
    ëª¨ë“  í¬ë¡¤ëŸ¬ë¥¼ í†µí•© ê´€ë¦¬í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤.
    ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œì—ì„œ í˜¸ì¶œí•©ë‹ˆë‹¤.
    """

    def __init__(self):
        self.korean_bar = KoreanBarCrawler()
        self.naver_blog = NaverBlogCrawler()
        self.youtube = YouTubeCrawler()
        self.legal_portal = LegalPortalCrawler()
        self._status = {
            "running": False,
            "source": "",
            "progress": "",
            "last_run": None,
            "last_result": None,
        }

    @property
    def status(self):
        return self._status

    def run(self, source: str = "all", **kwargs) -> dict:
        """
        í¬ë¡¤ë§ ì‹¤í–‰

        Args:
            source: "koreanbar" | "naver" | "youtube" | "portal" | "all"
            **kwargs: ê° í¬ë¡¤ëŸ¬ì— ì „ë‹¬í•  ì¶”ê°€ ì¸ì
                - max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜
                - keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
                - keywords: í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
                - legal_categories: ["ì´í˜¼", "ì „ì„¸ì‚¬ê¸°", ...] ë²•ë¥  ì¹´í…Œê³ ë¦¬
                - max_results: í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜

        Returns:
            {"added": int, "skipped": int, "total": int, "source": str, "duration": float}
        """
        self._status["running"] = True
        self._status["source"] = source
        self._status["progress"] = "ìˆ˜ì§‘ ì‹œì‘..."

        start_time = time.time()
        all_contacts = []
        legal_categories = kwargs.get("legal_categories")

        try:
            if source in ("koreanbar", "all"):
                self._status["progress"] = "ëŒ€í•œë³€í˜¸ì‚¬í˜‘íšŒ ìˆ˜ì§‘ ì¤‘..."
                contacts = self.korean_bar.crawl(
                    max_pages=kwargs.get("max_pages", 5),
                    keyword=kwargs.get("keyword", ""),
                )
                all_contacts.extend(contacts)

            if source in ("naver", "all"):
                self._status["progress"] = "ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì¤‘ (mainFrame ì „ëµ)..."
                contacts = self.naver_blog.crawl(
                    keywords=kwargs.get("keywords"),
                    max_results_per_keyword=kwargs.get("max_results", 10),
                    legal_categories=legal_categories,
                )
                all_contacts.extend(contacts)

            if source in ("youtube", "all"):
                self._status["progress"] = "ìœ íŠœë¸Œ ì˜ìƒ ì„¤ëª…ë€ ìˆ˜ì§‘ ì¤‘..."
                contacts = self.youtube.crawl(
                    keywords=kwargs.get("keywords"),
                    max_results_per_keyword=kwargs.get("max_results", 10),
                    legal_categories=legal_categories,
                )
                all_contacts.extend(contacts)

            if source in ("portal", "all"):
                self._status["progress"] = "ë²•ë¥  í¬í„¸ ìˆ˜ì§‘ ì¤‘..."
                contacts = self.legal_portal.crawl(
                    max_pages=kwargs.get("max_pages", 3),
                )
                all_contacts.extend(contacts)

            # DBì— ì €ì¥
            result = add_contacts(all_contacts)
            duration = time.time() - start_time

            result_info = {
                **result,
                "source": source,
                "duration": round(duration, 1),
                "collected_raw": len(all_contacts),
                "today_count": get_today_count(),
            }

            self._status["progress"] = f"ì™„ë£Œ: {result['added']}ê±´ ì¶”ê°€"
            self._status["last_run"] = datetime.now().isoformat()
            self._status["last_result"] = result_info

            print(f"\n{'='*50}")
            print(f"[í¬ë¡¤ë§ ìµœì¢… ê²°ê³¼]")
            print(f"  ì†ŒìŠ¤: {source}")
            print(f"  ìˆ˜ì§‘: {len(all_contacts)}ê±´ (ì›ë³¸)")
            print(f"  ì¶”ê°€: {result['added']}ê±´ (ì¤‘ë³µ ì œì™¸)")
            print(f"  ê±´ë„ˆëœ€: {result['skipped']}ê±´ (ì¤‘ë³µ)")
            print(f"  ì´ DB: {result['total']}ê±´")
            print(f"  ì†Œìš”ì‹œê°„: {duration:.1f}ì´ˆ")
            print(f"  ì˜¤ëŠ˜ ìˆ˜ì§‘: {result_info['today_count']}ê±´")
            print(f"{'='*50}\n")

            return result_info

        except Exception as e:
            self._status["progress"] = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            raise
        finally:
            self._status["running"] = False


# â”€â”€ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
crawler_instance = LawyerCrawler()


# â”€â”€ CLI ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="ë³€í˜¸ì‚¬ ì´ë©”ì¼ ìˆ˜ì§‘ í¬ë¡¤ëŸ¬ v2")
    parser.add_argument("--source", default="all", choices=["koreanbar", "naver", "youtube", "portal", "all"])
    parser.add_argument("--max-pages", type=int, default=3)
    parser.add_argument("--keyword", default="")
    parser.add_argument("--categories", nargs="*", default=None,
                        help="ë²•ë¥  ì¹´í…Œê³ ë¦¬: ì´í˜¼ ì „ì„¸ì‚¬ê¸° í˜•ì‚¬ ë¶€ë™ì‚° ìƒì† ë…¸ë™ ...")
    args = parser.parse_args()

    crawler = LawyerCrawler()
    result = crawler.run(
        source=args.source,
        max_pages=args.max_pages,
        keyword=args.keyword,
        legal_categories=args.categories,
    )
    print(f"\nìµœì¢…: {json.dumps(result, ensure_ascii=False, indent=2)}")
